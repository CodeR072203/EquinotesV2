import express from "express";
import http, { IncomingMessage } from "http";
import { WebSocketServer, WebSocket, RawData } from "ws";
import cors from "cors";

const app = express();
app.use(cors());
app.use(express.json());

app.get("/health", (_req, res) => {
  res.json({ ok: true, service: "equinotes-backend" });
});

const server = http.createServer(app);
const wss = new WebSocketServer({ server, path: "/ws" });

const WHISPER_URL = "ws://127.0.0.1:9090";

/**
 * Force using the custom model folder name in the container: /models/faster-whisper-small
 */
const FORCED_MODEL = "faster-whisper-small";

/**
 * CRITICAL FIX: WhisperLive does NOT accept "auto" as a language code!
 * Must use specific language codes like "en" or "tl"
 */
type WhisperLanguage = "en" | "tl"; // Changed from "auto" | "en" | "tl"

type WhisperSegment = { text?: unknown };
type WhisperLikeMessage = {
  text?: unknown;
  transcript?: unknown;
  result?: { text?: unknown };
  data?: { text?: unknown };
  segments?: WhisperSegment[];
  message?: unknown;
  status?: unknown;
  uid?: unknown;
};

type ClientControlMessage =
  | {
      type: "config";
      language?: WhisperLanguage;
      model?: string; // ignored / mapped to FORCED_MODEL
      translate?: boolean;
      use_vad?: boolean;
    }
  | { type: "ping" };

type Channel = "client" | "agent" | "unknown";

type OutgoingJson =
  | { type: "info"; channel: Channel; message: string }
  | { type: "status"; channel: Channel; message: string }
  | { type: "transcript"; channel: Channel; text: string }
  | { type: "raw"; channel: Channel; text: string };

function tryExtractTranscript(raw: string): string | null {
  try {
    const parsed: unknown = JSON.parse(raw);
    if (typeof parsed !== "object" || parsed === null) return null;

    const obj = parsed as WhisperLikeMessage;

    const segText =
      Array.isArray(obj.segments)
        ? obj.segments
            .map((s) => (typeof s.text === "string" ? s.text : ""))
            .join(" ")
        : null;

    const candidates: unknown[] = [
      segText,
      obj.text,
      obj.transcript,
      obj.result?.text,
      obj.data?.text,
      obj.message,
      obj.status,
    ];

    for (const c of candidates) {
      if (typeof c === "string") {
        const t = c.trim();
        if (t.length > 0) return t;
      }
    }
    return null;
  } catch {
    const t = raw.trim();
    return t.length > 0 ? t : null;
  }
}

function isServerReady(raw: string): boolean {
  if (raw.includes("SERVER_READY")) return true;
  try {
    const parsed: unknown = JSON.parse(raw);
    if (typeof parsed !== "object" || parsed === null) return false;
    const obj = parsed as Record<string, unknown>;
    return obj.message === "SERVER_READY" || obj.status === "SERVER_READY";
  } catch {
    return false;
  }
}

function safeParseControlMessage(text: string): ClientControlMessage | null {
  try {
    const parsed: unknown = JSON.parse(text);
    if (typeof parsed !== "object" || parsed === null) return null;

    const obj = parsed as Record<string, unknown>;
    if (obj.type === "config") {
      const language = obj.language;
      const model = obj.model;
      const translate = obj.translate;
      const use_vad = obj.use_vad;

      // Only accept "en" or "tl" - WhisperLive doesn't accept "auto"
      const langOk = language === "en" || language === "tl";

      return {
        type: "config",
        language: langOk ? (language as WhisperLanguage) : "en", // Default to "en"
        model: typeof model === "string" ? model : undefined,
        translate: typeof translate === "boolean" ? translate : undefined,
        use_vad: typeof use_vad === "boolean" ? use_vad : undefined,
      };
    }

    if (obj.type === "ping") return { type: "ping" };
    return null;
  } catch {
    return null;
  }
}

/**
 * WhisperLive init message. CRITICAL: must use valid language codes.
 */
function buildWhisperInit(params: {
  uid: string;
  language: WhisperLanguage;
  translate: boolean;
  use_vad: boolean;
}) {
  const { uid, language, translate, use_vad } = params;
  return {
    uid,
    model: FORCED_MODEL,
    language, // MUST be valid language code like "en" or "tl", NOT "auto"
    task: translate ? "translate" : "transcribe",
    use_vad,
    initial_prompt: "Transcribe the audio to text.", // Simplified prompt
  };
}

function rawToBuffer(data: RawData): Buffer {
  if (Buffer.isBuffer(data)) return data;
  if (data instanceof ArrayBuffer) return Buffer.from(new Uint8Array(data));
  if (Array.isArray(data)) return Buffer.concat(data as Buffer[]);
  return Buffer.from(String(data));
}

/**
 * Convert PCM16 bytes (Int16LE) -> Float32LE bytes (normalized -1..1).
 * WhisperLive expects float32 samples.
 */
function pcm16leToFloat32Bytes(pcmBytes: Buffer): Buffer {
  const evenLen = pcmBytes.length - (pcmBytes.length % 2);
  const sampleCount = evenLen / 2;

  const floats = new Float32Array(sampleCount);
  for (let i = 0; i < sampleCount; i++) {
    const s = pcmBytes.readInt16LE(i * 2);
    floats[i] = s < 0 ? s / 32768 : s / 32767;
  }

  return Buffer.from(new Uint8Array(floats.buffer));
}

function parseChannelFromReq(req: IncomingMessage): Channel {
  try {
    const url = new URL(req.url ?? "", "http://localhost");
    const ch = (url.searchParams.get("channel") ?? "").toLowerCase();
    if (ch === "client") return "client";
    if (ch === "agent") return "agent";
    return "unknown";
  } catch {
    return "unknown";
  }
}

wss.on("connection", (clientSocket: WebSocket, req: IncomingMessage) => {
  const channel: Channel = parseChannelFromReq(req);

  console.log(`Frontend WS client connected (channel=${channel})`);

  // Defaults: Use "en" instead of "auto" (WhisperLive doesn't accept "auto")
  let language: WhisperLanguage = "en";
  let translate = false;
  let use_vad = false;

  let micStreaming = false;

  // Track whether we've received ANY real binary audio yet (per-channel)
  // This is critical because the frontend sends END_OF_AUDIO immediately on connect.
  let hasReceivedAnyAudio = false;

  // Audio rate logging (from browser -> backend)
  let bytesFromBrowser = 0;
  let lastLog = Date.now();

  // Audio rate + peak logging (backend -> WhisperLive)
  let bytesToWhisper = 0;
  let peakAbs = 0;
  let lastWhisperLog = Date.now();

  // Whisper socket + state
  let whisperSocket: WebSocket | null = null;
  let whisperReady = false;

  // Track if we've sent END_OF_AUDIO to WhisperLive
  let endOfAudioSent = false;

  // Track messages from WhisperLive for debugging
  let whisperMessagesReceived = 0;

  // PCM input queue (PCM16LE from browser)
  let pcmQueue: Buffer = Buffer.alloc(0);

  // Float32 chunks waiting for SERVER_READY
  const pendingFloat32: Buffer[] = [];

  // Reconnect control
  let nextReconnectAt = 0;
  let backoffMs = 500;
  const BACKOFF_MAX_MS = 10_000;

  // Prevent "infinite reject loop"
  let consecutiveEarlyCloses = 0;
  const MAX_EARLY_CLOSES = 6;

  // Graceful stop control (do NOT close immediately after END_OF_AUDIO)
  let stopRequested = false;
  let stopDeadlineMs = 0;
  let stopTimer: NodeJS.Timeout | null = null;

  function nowMs() {
    return Date.now();
  }

  function sendToFrontendSafe(msg: string | Buffer) {
    try {
      if (clientSocket.readyState === WebSocket.OPEN) clientSocket.send(msg);
    } catch {
      // ignore
    }
  }

  function sendJsonSafe(payload: OutgoingJson) {
    sendToFrontendSafe(JSON.stringify(payload));
  }

  function info(message: string) {
    sendJsonSafe({ type: "info", channel, message });
  }

  function status(message: string) {
    sendJsonSafe({ type: "status", channel, message });
  }

  function transcript(text: string) {
    sendJsonSafe({ type: "transcript", channel, text });
  }

  function raw(text: string) {
    sendJsonSafe({ type: "raw", channel, text });
  }

  info("Connected to EquiNotes backend WebSocket");

  function clearStopTimer() {
    if (stopTimer) {
      clearInterval(stopTimer);
      stopTimer = null;
    }
  }

  function safeCloseWs(ws: WebSocket | null, code: number, reason: string) {
    if (!ws) return;
    try {
      if (ws.readyState === WebSocket.CONNECTING) {
        ws.terminate();
        return;
      }
      if (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CLOSING) {
        ws.close(code, reason);
      }
    } catch {
      // ignore
    }
  }

  function closeWhisperLocalState() {
    whisperReady = false;
    pendingFloat32.length = 0;
    whisperSocket = null;
    endOfAudioSent = false;
    console.log(`WhisperLive local state cleared (channel=${channel}), total messages: ${whisperMessagesReceived}`);
  }

  function closeWhisperSafe(reason: string) {
    whisperReady = false;

    const ws = whisperSocket;
    closeWhisperLocalState();

    if (!ws) return;
    safeCloseWs(ws, 1000, reason);
  }

  function sendEndOfAudioToWhisper(ws: WebSocket) {
    // WhisperLive expects literal bytes END_OF_AUDIO
    try {
      if (ws.readyState === WebSocket.OPEN && !endOfAudioSent) {
        console.log(`[IMPORTANT] Sending END_OF_AUDIO to WhisperLive (channel=${channel})`);
        ws.send(Buffer.from("END_OF_AUDIO"));
        endOfAudioSent = true;
        
        // Log after sending END_OF_AUDIO
        setTimeout(() => {
          console.log(`[INFO] 2 seconds after END_OF_AUDIO (channel=${channel}), messages: ${whisperMessagesReceived}`);
        }, 2000);
      }
    } catch (error) {
      console.log(`Error sending END_OF_AUDIO: ${error}`);
    }
  }

  function updateWhisperDiagnostics(floatBytes: Buffer) {
    bytesToWhisper += floatBytes.length;

    // compute peak abs quickly
    const len = floatBytes.length - (floatBytes.length % 4);
    for (let i = 0; i < len; i += 4) {
      const v = floatBytes.readFloatLE(i);
      const a = Math.abs(v);
      if (a > peakAbs) peakAbs = a;
    }

    const t = nowMs();
    if (t - lastWhisperLog >= 1000) {
      console.log(
        `to whisper: ~${bytesToWhisper} bytes/sec (channel=${channel}), peakAbs=${peakAbs.toFixed(
          4
        )}, msgs=${whisperMessagesReceived}`
      );
      bytesToWhisper = 0;
      peakAbs = 0;
      lastWhisperLog = t;
    }
  }

  function scheduleForcedStopIfNeeded() {
    if (!stopRequested) return;
    if (stopTimer) return;

    stopTimer = setInterval(() => {
      if (!stopRequested) {
        clearStopTimer();
        return;
      }

      // If WhisperLive already closed, nothing to do.
      if (!whisperSocket) {
        stopRequested = false;
        clearStopTimer();
        return;
      }

      // Wait until deadline, then terminate to avoid hanging forever.
      if (nowMs() >= stopDeadlineMs) {
        const ws = whisperSocket;
        closeWhisperLocalState();
        safeCloseWs(ws, 1000, "client_stop_timeout");
        stopRequested = false;
        clearStopTimer();
        status("WhisperLive stop timeout; connection closed");
      }
    }, 250);
  }

  function flushPendingIfReady(ws: WebSocket) {
    if (!whisperReady) return;
    while (pendingFloat32.length > 0 && ws.readyState === WebSocket.OPEN) {
      const chunk = pendingFloat32.shift();
      if (!chunk) break;
      updateWhisperDiagnostics(chunk);
      ws.send(chunk);
    }
  }

  function ensureWhisperConnected() {
    if (!micStreaming) return;

    // If we're stopping, do not reconnect.
    if (stopRequested) return;

    const t = nowMs();
    if (nextReconnectAt > t) return;

    if (
      whisperSocket &&
      (whisperSocket.readyState === WebSocket.CONNECTING ||
        whisperSocket.readyState === WebSocket.OPEN ||
        whisperSocket.readyState === WebSocket.CLOSING)
    ) {
      return;
    }

    const ws = new WebSocket(WHISPER_URL);
    whisperSocket = ws;
    whisperReady = false;
    whisperMessagesReceived = 0;

    const uid = `equinotes-${channel}-${Date.now()}-${Math.random()
      .toString(16)
      .slice(2)}`;

    let readySeenForThisSocket = false;

    ws.on("open", () => {
      console.log(`Connected to WhisperLive: ${WHISPER_URL} (channel=${channel})`);
      status(`Connected to WhisperLive (${WHISPER_URL})`);

      const initMsg = buildWhisperInit({
        uid,
        language,
        translate,
        use_vad,
      });

      ws.send(JSON.stringify(initMsg));
      console.log(`Sent WhisperLive init (channel=${channel}):`, JSON.stringify(initMsg));
    });

    ws.on("message", (msg, isBinary) => {
      if (isBinary) {
        const b = Buffer.isBuffer(msg) ? msg : rawToBuffer(msg as any);
        sendToFrontendSafe(b);
        return;
      }

      whisperMessagesReceived++;
      const text = msg.toString();
      console.log(`WhisperLive msg#${whisperMessagesReceived} (channel=${channel}): ${text.substring(0, 200)}`);
      
      // DEBUG: Parse and check for transcripts
      try {
        const parsed = JSON.parse(text);
        console.log(`DEBUG Parsed keys:`, Object.keys(parsed));
        
        if (parsed.text && typeof parsed.text === 'string') {
          console.log(`ðŸŽ‰ TRANSCRIPT FOUND in 'text': "${parsed.text}"`);
        }
        if (parsed.transcript && typeof parsed.transcript === 'string') {
          console.log(`ðŸŽ‰ TRANSCRIPT FOUND in 'transcript': "${parsed.transcript}"`);
        }
        if (parsed.result && parsed.result.text && typeof parsed.result.text === 'string') {
          console.log(`ðŸŽ‰ TRANSCRIPT FOUND in 'result.text': "${parsed.result.text}"`);
        }
        if (parsed.segments && Array.isArray(parsed.segments)) {
          console.log(`DEBUG Found ${parsed.segments.length} segments`);
          parsed.segments.forEach((seg: any, i: number) => {
            if (seg.text && typeof seg.text === 'string') {
              console.log(`  Segment ${i}: "${seg.text}"`);
            }
          });
        }
      } catch (e) {
        if (text.trim().length > 0 && !text.includes('SERVER_READY')) {
          console.log(`DEBUG Non-JSON message: "${text}"`);
        }
      }
      
      raw(text);

      if (!readySeenForThisSocket && isServerReady(text)) {
        readySeenForThisSocket = true;
        whisperReady = true;

        consecutiveEarlyCloses = 0;
        backoffMs = 500;
        nextReconnectAt = 0;

        status("WhisperLive is ready");
        flushPendingIfReady(ws);

        // If stop was requested before we became ready, send END now.
        if (stopRequested) {
          sendEndOfAudioToWhisper(ws);
          scheduleForcedStopIfNeeded();
        }
        return;
      }

      const t = tryExtractTranscript(text);
      if (t && t.trim().length > 0) {
        console.log(`ðŸŽ‰ TRANSCRIPT EXTRACTED (channel=${channel}): "${t}"`);
        transcript(t);
      }
    });

    ws.on("close", (code, reason) => {
      const rsn = reason?.toString() ?? "";
      console.log(
        `WhisperLive WS closed: code=${code} reason=${rsn} (channel=${channel}, msgs=${whisperMessagesReceived})`
      );

      const closedBeforeReady = !readySeenForThisSocket;

      whisperReady = false;
      if (whisperSocket === ws) whisperSocket = null;

      // If we were intentionally stopping, don't reconnect.
      if (stopRequested) {
        stopRequested = false;
        clearStopTimer();
        status(`WhisperLive closed (${rsn || "closed"})`);
        return;
      }

      if (!micStreaming) return;

      if (closedBeforeReady) {
        consecutiveEarlyCloses += 1;

        if (consecutiveEarlyCloses >= MAX_EARLY_CLOSES) {
          status(
            "WhisperLive keeps closing before SERVER_READY. Check WhisperLive logs for init errors (model/language/etc)."
          );
          micStreaming = false;
          return;
        }

        nextReconnectAt = nowMs() + backoffMs;
        backoffMs = Math.min(BACKOFF_MAX_MS, backoffMs * 2);

        status(`WhisperLive closed early; reconnecting soon...`);
      } else {
        nextReconnectAt = nowMs() + 750;
        status("WhisperLive connection closed; reconnecting...");
      }
    });

    ws.on("error", (err) => {
      console.error(`WhisperLive WS error (channel=${channel}):`, err);
      whisperReady = false;
      if (whisperSocket === ws) whisperSocket = null;

      // If we were intentionally stopping, don't reconnect.
      if (stopRequested) {
        stopRequested = false;
        clearStopTimer();
        status("WhisperLive error during stop; connection closed");
        return;
      }

      if (micStreaming) {
        nextReconnectAt = nowMs() + backoffMs;
        backoffMs = Math.min(BACKOFF_MAX_MS, backoffMs * 2);
        status("WhisperLive error; backing off reconnect...");
      }
    });
  }

  function drainPcmToWhisper() {
    const PCM_CHUNK_BYTES = 16384; // Increased from 4096 for better processing

    while (pcmQueue.length >= PCM_CHUNK_BYTES) {
      const pcmChunk = pcmQueue.subarray(0, PCM_CHUNK_BYTES);
      pcmQueue = Buffer.from(pcmQueue.subarray(PCM_CHUNK_BYTES));

      const floatBytes = pcm16leToFloat32Bytes(Buffer.from(pcmChunk));

      const ws = whisperSocket;
      if (ws && ws.readyState === WebSocket.OPEN && whisperReady) {
        updateWhisperDiagnostics(floatBytes);
        ws.send(floatBytes);
      } else {
        if (pendingFloat32.length < 400) pendingFloat32.push(floatBytes);
      }
    }
  }

  function flushPcmTailToPending() {
    try {
      const evenLen = pcmQueue.length - (pcmQueue.length % 2);
      if (evenLen > 0) {
        const pcmTail = pcmQueue.subarray(0, evenLen);
        pcmQueue = Buffer.alloc(0);

        const floatTail = pcm16leToFloat32Bytes(Buffer.from(pcmTail));
        const ws = whisperSocket;
        if (ws && ws.readyState === WebSocket.OPEN && whisperReady) {
          updateWhisperDiagnostics(floatTail);
          ws.send(floatTail);
        } else {
          if (pendingFloat32.length < 400) pendingFloat32.push(floatTail);
        }
      } else {
        pcmQueue = Buffer.alloc(0);
      }
    } catch {
      // ignore
    }
  }

  function requestStopWhisper() {
    // Stop streaming new audio, but keep the WhisperLive socket alive briefly
    // so it can emit final segments.
    micStreaming = false;

    // flush any buffered PCM -> float32
    flushPcmTailToPending();

    // mark stop request and set deadline
    stopRequested = true;
    stopDeadlineMs = nowMs() + 6000; // allow up to 6s for final results

    // If connected & ready, send END immediately; otherwise it will be sent on SERVER_READY.
    const ws = whisperSocket;
    if (ws && ws.readyState === WebSocket.OPEN && whisperReady) {
      sendEndOfAudioToWhisper(ws);
    }

    scheduleForcedStopIfNeeded();
  }

  clientSocket.on("message", (data, isBinary) => {
    if (!isBinary) {
      const text = data.toString();

      if (text === "END_OF_AUDIO") {
        // CRITICAL: frontend sends END_OF_AUDIO immediately on connect.
        // Ignore until we have received at least one real audio frame.
        if (!hasReceivedAnyAudio) {
          console.log(
            `Ignoring END_OF_AUDIO (no audio received yet) (channel=${channel})`
          );
          status("Ignoring END_OF_AUDIO (no audio received yet)");
          return;
        }

        console.log(`Received END_OF_AUDIO from frontend (channel=${channel})`);

        status("END_OF_AUDIO received");
        requestStopWhisper();
        return;
      }

      const ctrl = safeParseControlMessage(text);
      if (ctrl?.type === "ping") {
        raw("pong");
        return;
      }

      if (ctrl?.type === "config") {
        // Only accept "en" or "tl" - WhisperLive doesn't accept "auto"
        language = ctrl.language === "tl" ? "tl" : "en";
        translate = ctrl.translate ?? translate;
        use_vad = ctrl.use_vad ?? use_vad;

        const msg = `Config set (forced model): model=${FORCED_MODEL}, lang=${language}, translate=${translate}, vad=${use_vad}`;
        console.log(`${msg} (channel=${channel})`);
        status(msg);

        // Apply live by reconnecting WhisperLive immediately.
        // If we are stopping, cancel stop and reconnect.
        stopRequested = false;
        clearStopTimer();

        closeWhisperSafe("config_change");
        if (micStreaming) ensureWhisperConnected();
        return;
      }

      raw(`Ignored text: ${text}`);
      return;
    }

    // binary audio frame from browser
    const buf = rawToBuffer(data);

    // Detect common container/codec headers (means NOT raw PCM16)
    if (buf.length >= 4) {
      const head4 = buf.subarray(0, 4).toString("ascii");
      const head4hex = buf.subarray(0, 4).toString("hex");
      if (
        head4 === "OggS" ||
        head4 === "RIFF" ||
        head4 === "fLaC" ||
        head4 === "ID3" ||
        head4hex === "1a45dfa3" // EBML (webm/mkv)
      ) {
        console.log(
          `WARNING: received encoded/container audio (${head4}/${head4hex}) - WhisperLive expects raw PCM frames`
        );
      }
    }

    const evenLen = buf.length - (buf.length % 2);
    if (evenLen <= 0) return;

    // Mark that we've received real audio (so END_OF_AUDIO becomes meaningful)
    hasReceivedAnyAudio = true;

    // If we had a stop requested, new audio implies a new session; cancel stop.
    if (stopRequested) {
      stopRequested = false;
      clearStopTimer();
    }

    micStreaming = true;

    bytesFromBrowser += evenLen;
    const t = nowMs();
    if (t - lastLog >= 1000) {
      const kbPerSec = Math.round(bytesFromBrowser / 1024);
      console.log(`audio in: ~${kbPerSec} KB/sec (channel=${channel})`);
      bytesFromBrowser = 0;
      lastLog = t;
    }

    const slice = Buffer.from(buf.subarray(0, evenLen));
    pcmQueue = pcmQueue.length === 0 ? slice : Buffer.concat([pcmQueue, slice]);

    ensureWhisperConnected();
    drainPcmToWhisper();
  });

  clientSocket.on("close", () => {
    console.log(`Frontend WS client disconnected (channel=${channel})`);

    micStreaming = false;
    hasReceivedAnyAudio = false;
    pcmQueue = Buffer.alloc(0);
    pendingFloat32.length = 0;

    stopRequested = false;
    clearStopTimer();

    // Best-effort tell WhisperLive the stream ended, then close.
    try {
      const ws = whisperSocket;
      if (ws && ws.readyState === WebSocket.OPEN) {
        sendEndOfAudioToWhisper(ws);
      }
    } catch {
      // ignore
    }

    closeWhisperSafe("frontend_disconnect");
  });

  clientSocket.on("error", (err) => {
    console.error(`Frontend WS error (channel=${channel}):`, err);

    micStreaming = false;
    hasReceivedAnyAudio = false;
    pcmQueue = Buffer.alloc(0);
    pendingFloat32.length = 0;

    stopRequested = false;
    clearStopTimer();

    closeWhisperSafe("frontend_error");
  });
});

const PORT = 3001;
server.listen(PORT, () => {
  console.log(`EquiNotes backend running at http://0.0.0.0:${PORT}`);
  console.log(`WebSocket endpoint: ws://<server-ip>:${PORT}/ws`);
  console.log(`IMPORTANT FIX: Using language="en" (WhisperLive doesn't accept "auto")`);
});